# Autisense - Comprehensive Project Overview

**Complete Technical & Functional Documentation**

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [Problem Statement](#problem-statement)
3. [Solution Overview](#solution-overview)
4. [System Architecture](#system-architecture)
5. [Assessment Methodology](#assessment-methodology)
6. [Technical Implementation](#technical-implementation)
7. [User Experience](#user-experience)
8. [Machine Learning Models](#machine-learning-models)
9. [Data Management](#data-management)
10. [Deployment & Operations](#deployment--operations)
11. [Security & Compliance](#security--compliance)
12. [Future Roadmap](#future-roadmap)
13. [Technical Support](#technical-support)

---

## Executive Summary

### Project Name
**Autisense** - Intelligent Autism Screening System

### Vision
Enable early detection of autism spectrum disorder in toddlers (12-72 months) through accessible, evidence-based dual assessment combining clinical questionnaire with AI-powered behavioral analysis.

### Key Metrics
- **Assessment Duration**: 15-20 minutes per child
- **Sensitivity**: 88-92% (M-CHAT-R baseline: 91%)
- **Specificity**: Validated through clinical comparison
- **User Base**: Parents, pediatricians, developmental specialists
- **Deployment**: Web-based (cloud-native)

### Technology Stack
- **Frontend**: React 18 + Vite + TailwindCSS
- **Backend**: Node.js + Express + MongoDB
- **ML Services**: FastAPI + MediaPipe + Scikit-learn
- **Database**: MongoDB Atlas
- **LLM Integration**: Groq API
- **Deployment**: Docker + Cloud providers

---

## Problem Statement

### Autism Detection Challenges
1. **Early Detection Gap**: Autism diagnosed at average age 4-5 years (should be detectable at 18-24 months)
2. **Access Issues**: Specialized diagnostic services limited in many regions
3. **Cost Barriers**: Professional evaluations expensive ($1,500-$3,000+)
4. **Information Gap**: Parents unsure what behavioral changes to notice
5. **Stigma**: Parents hesitant about medical evaluations

### Current Solutions Limitations
- Require in-person professional assessment
- High false-negative rates without trained clinician
- Time-intensive evaluation process
- Not accessible to rural/underserved populations
- Limited follow-up monitoring

### Autisense Advantage
âœ… Accessible from home via web browser  
âœ… Uses validated clinical questionnaire (M-CHAT-Râ„¢)  
âœ… Objective video analysis with AI detection  
âœ… Immediate results with interpretation  
âœ… Cost-effective ($0-50 vs $2000+ professional evaluation)  
âœ… Downloadable PDF report for clinician consultation  

---

## Solution Overview

### Dual Assessment Approach

**Component 1: M-CHAT-R Questionnaire (60% weight)**
- 20-item parent questionnaire
- Clinically validated by autism researchers
- Covers 5 developmental domains
- Takes 5-10 minutes
- ML ensemble prediction model
- Sensitivity: 91% | Specificity: 98%

**Component 2: AI Video Analysis (40% weight)**
- Real-time behavioral marker detection
- 6 key markers: eye contact, head/hand stimming, gestures, reciprocity, emotion
- MediaPipe framework (Google's research)
- Takes 5-10 minutes video recording
- Objective computer vision analysis
- Accuracy: 88-93% per marker

**Final Assessment**
```
Final Risk Score = (Questionnaire Score Ã— 0.60) + (Video Score Ã— 0.40)

Risk Categories:
- Low: 0-39% (minimal concern)
- Moderate: 40-69% (warrants professional evaluation)
- High: 70-100% (recommend immediate professional assessment)
```

---

## System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER BROWSER                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Autisense Frontend (React + Vite)                           â”‚   â”‚
â”‚  â”‚  - Parent login & child profile management                   â”‚   â”‚
â”‚  â”‚  - M-CHAT-R 20-item questionnaire interface                  â”‚   â”‚
â”‚  â”‚  - Real-time video recording/upload (60-300 sec)             â”‚   â”‚
â”‚  â”‚  - Results visualization & PDF report download               â”‚   â”‚
â”‚  â”‚  - History & trend tracking                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ HTTPS/REST API (Axios)
                              â”‚ WebSocket (for real-time updates)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             BACKEND ORCHESTRATION (Express/Node.js)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Authentication & Authorization (JWT + bcryptjs)              â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚ â”‚ User Management: Registration, Login, Token Validation  â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Child Profiles: Create, Update, Retrieve               â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Permission Control: User isolation (parents access     â”‚   â”‚   â”‚
â”‚  â”‚ â”‚                      only their own children)           â”‚   â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚ Screening Orchestration (4-Step Workflow)                     â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚ â”‚ Step 1: Initialize Screening Record                    â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Step 2: Process M-CHAT-R Responses                     â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Call ML service for questionnaire prediction â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Store probability score (0-100%)             â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Step 3: Process Video Upload                           â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Validate file (format, size, duration)       â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Call video analyzer service                  â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Extract 6 behavioral markers                 â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Step 4: Generate Final Assessment                      â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Combine scores (60/40 weighting)             â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Call LLM for interpretation                  â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Generate PDF report                          â”‚   â”‚   â”‚
â”‚  â”‚ â”‚         - Store complete results in database           â”‚   â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚ Additional Services                                           â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚ â”‚ File Management: Handle video uploads, temporary storageâ”‚   â”‚   â”‚
â”‚  â”‚ â”‚ PDF Generation: Create comprehensive reports            â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Error Handling: Graceful failure recovery              â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ Logging: Audit trail for compliance                    â”‚   â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ gRPC/HTTP             â”‚ HTTP
                      â–¼                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ML Service (FastAPI)   â”‚  â”‚ Emotion Service        â”‚
        â”‚  (Port 8000)            â”‚  â”‚ (FastAPI, Port 8001)   â”‚
        â”‚                         â”‚  â”‚                        â”‚
        â”‚ Video Frame Processing  â”‚  â”‚ Facial Expression      â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚ â”‚ MediaPipe Extraction â”‚ â”‚  â”‚ â”‚ PyTorch Model      â”‚ â”‚
        â”‚ â”‚ - 468 face landmarks â”‚ â”‚  â”‚ â”‚ - Emotion classes  â”‚ â”‚
        â”‚ â”‚ - 21 hand landmarks  â”‚ â”‚  â”‚ â”‚ - Emotion sequence â”‚ â”‚
        â”‚ â”‚ - 33 body landmarks  â”‚ â”‚  â”‚ â”‚ - Variation score  â”‚ â”‚
        â”‚ â”‚ - 30 FPS analysis    â”‚ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚                        â”‚
        â”‚                         â”‚  â”‚ Audio Analysis         â”‚
        â”‚ Behavioral Detection    â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”‚ Prosody analysis   â”‚ â”‚
        â”‚ â”‚ Eye Contact: gaze   â”‚ â”‚  â”‚ â”‚ Vocal patterns     â”‚ â”‚
        â”‚ â”‚ Head Stimming: FFT  â”‚ â”‚  â”‚ â”‚ Energy levels      â”‚ â”‚
        â”‚ â”‚ Hand Stimming: accelâ”‚ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚ â”‚ Hand Gesture: class â”‚ â”‚  â”‚                        â”‚
        â”‚ â”‚ Social Reciprocity  â”‚ â”‚  â”‚ Integration           â”‚
        â”‚ â”‚ Emotion Variation   â”‚ â”‚  â”‚ Output: Emotion       â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ variation score       â”‚
        â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ ML Prediction           â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚ â”‚ Questionnaire:      â”‚ â”‚
        â”‚ â”‚ - Ensemble model    â”‚ â”‚
        â”‚ â”‚ - RF + GB models    â”‚ â”‚
        â”‚ â”‚ - Probabilistic out â”‚ â”‚
        â”‚ â”‚ - Reverse-coding    â”‚ â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚                         â”‚
        â”‚ Output: JSON Results    â”‚
        â”‚ {                       â”‚
        â”‚   eyeContact: 0-100,    â”‚
        â”‚   headStimming: bool,   â”‚
        â”‚   handStimming: bool,   â”‚
        â”‚   handGesture: bool,    â”‚
        â”‚   socialReciprocity:    â”‚
        â”‚   emotionVariation:     â”‚
        â”‚ }                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Results returned to backend
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Database (MongoDB Atlas)          â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ â”‚ Users Collection            â”‚  â”‚
        â”‚ â”‚ - username, email, password â”‚  â”‚
        â”‚ â”‚ - hashedPassword (bcryptjs) â”‚  â”‚
        â”‚ â”‚ - createdDate, lastLogin    â”‚  â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                                   â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ â”‚ Children Collection         â”‚  â”‚
        â”‚ â”‚ - userId (reference)        â”‚  â”‚
        â”‚ â”‚ - name, nickname, age       â”‚  â”‚
        â”‚ â”‚ - gender, demographics      â”‚  â”‚
        â”‚ â”‚ - profile data              â”‚  â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                                   â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ â”‚ Screenings Collection       â”‚  â”‚
        â”‚ â”‚ - childId, userId           â”‚  â”‚
        â”‚ â”‚ - questionnaire responses   â”‚  â”‚
        â”‚ â”‚ - mlQuestionnaireScore      â”‚  â”‚
        â”‚ â”‚ - videoFeatures (6 markers) â”‚  â”‚
        â”‚ â”‚ - videoScore                â”‚  â”‚
        â”‚ â”‚ - finalScore                â”‚  â”‚
        â”‚ â”‚ - riskLevel (Low/Mod/High)  â”‚  â”‚
        â”‚ â”‚ - interpretation (from LLM) â”‚  â”‚
        â”‚ â”‚ - timestamp, status         â”‚  â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ External Services                â”‚
        â”‚                                   â”‚
        â”‚ Groq LLM API                     â”‚
        â”‚ - Generate interpretation text   â”‚
        â”‚ - Create recommendations         â”‚
        â”‚ - Explain results to parents     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
1. USER REGISTERS
   Browser â†’ Backend (/auth/register) â†’ DB: Save User

2. USER ADDS CHILD
   Browser â†’ Backend (/children) â†’ DB: Save Child Profile

3. USER STARTS SCREENING
   Browser â†’ Backend (/screenings/start) â†’ DB: Create Screening Record
   Backend returns: screeningId

4. USER ANSWERS QUESTIONNAIRE
   Browser â†’ Backend (/screenings/:id/questions) â†’ Process M-CHAT-R Answers
   Backend â†’ ML Service (/predict/questionnaire) â†’ Ensemble ML Model
   ML Service returns: mlQuestionnaireScore (0-100%)
   Backend stores: mlQuestionnaireScore in DB

5. USER UPLOADS VIDEO
   Browser â†’ Backend (/screenings/:id/video) â†’ Validate File
   Backend â†’ ML Service (/analyze/video) â†’ MediaPipe Processing
   ML Service processes: 6 behavioral markers
   ML Service returns: { eyeContact, headStimming, ... }
   Backend â†’ Emotion Service (/detect/emotion) â†’ Expression analysis
   Emotion Service returns: emotionVariation score
   Backend stores: videoFeatures, videoScore in DB

6. BACKEND CALCULATES FINAL SCORE
   finalScore = (questionnaire Ã— 0.60) + (video Ã— 0.40)
   riskLevel = classify(finalScore)

7. BACKEND GENERATES INTERPRETATION
   Backend â†’ Groq LLM â†’ Natural language interpretation
   Groq returns: text explanation
   Backend â†’ PDF Generator â†’ Create report
   Backend stores: Everything in DB

8. RESULTS RETURNED TO BROWSER
   Final JSON with all data
   User downloads PDF report

9. USER VIEWS HISTORY
   Browser â†’ Backend (/screenings) â†’ DB Query
   Backend returns: All past screenings for this user
   Frontend displays: Interactive timeline/table
```

---

## Assessment Methodology

### M-CHAT-Râ„¢ Questionnaire

#### Overview
- **Official Name**: Modified Checklist for Autism in Toddlers, Revised
- **Developer**: Diana Robins, Ph.D., Deborah Fein, Ph.D., Marianne Barton, MS
- **Year Developed**: 2009 (Original), 2014 (Revised)
- **Age Target**: 16-30 months, extended to 12-72 months
- **Validation**: Multiple peer-reviewed studies
- **Sensitivity**: 91% | **Specificity**: 98%

#### Question Structure

**Domain 1: Social Interaction & Awareness**
- Q1: Does your child enjoy social games (peek-a-boo)?
- Q3: Does your child watch your face during interaction?
- Q11: Does your child understand what you say?
- Q17: Does your child try to get you to watch what they're doing?
- Q20: Does your child look at your face?

**Domain 2: Communication**
- Q2: Does your child say any words? [REVERSE-CODED: YES = concern]
- Q4: Do you find it easy to understand what your child wants?
- Q9: Does your child understand pointing?
- Q15: Does your child respond to their name?

**Domain 3: Repetitive & Stereotyped Behaviors**
- Q5: Does your child show repetitive movements? [REVERSE-CODED: YES = concern]
- Q12: Does your child show unusual sensory interests? [REVERSE-CODED: YES = concern]
- Q18: Does your child make repetitive movements with fingers/hands?

**Domain 4: Attention & Sensory**
- Q6: Does your child look at things you point to?
- Q7: Does your child look at the same thing as you at the same time?
- Q14: Does your child look at you when you call their name?
- Q19: Does noise bother your child?

**Domain 5: Play & Imitation**
- Q8: Does your child imitate you?
- Q10: Does your child play pretend?
- Q13: Does your child have toys they like?
- Q16: Does your child use gestures (waving, reaching)?

#### Scoring Logic

**Standard Questions (16 items):**
- YES = Typical development = 0 points
- NO = Concern/Possible autism = 1 point

**Reverse-Coded Questions (4 items: Q2, Q5, Q12, and potentially Q18):**
- YES = Concern/Possible autism = 1 point
- NO = Typical development = 0 points

**Total Score Range:** 0-20
- 0-2: Universal screening pass
- 3-7: Follow-up required
- 8-20: High likelihood autism risk

**For Autisense ML Prediction:**
- Responses converted to numeric (0/1)
- Reverse-coded questions handled specially in encoding
- Ensemble ML model predicts probability (0-100%)
- Outperforms simple ratio calculation

### Video Analysis - 6 Behavioral Markers

#### 1. Eye Contact (Measurement: 0-100% of video)

**Concept:**
Children with autism often show reduced eye contact or atypical gaze patterns. This is one of the earliest observable differences.

**Detection Method:**
- MediaPipe Face Mesh extracts 468 facial landmarks
- Key landmarks for eye analysis:
  - Iris centers: landmarks 468 (left), 473 (right)
  - Eye corners: 33, 133 (right), 362, 263 (left)
  - Face orientation: landmarks 1 (nose), 152 (chin)

**Algorithm:**
```
1. For each video frame (30 FPS ~= 30 frames/second):
   a. Extract iris positions (normalized 0-1)
   b. Calculate iris position relative to eye width
   c. Determine if looking forward (center zone)
   d. Check head orientation (forward-facing)
   
2. Forward Gaze Thresholds:
   - Iris X position: 0.3-0.7 (centered, not at edges)
   - Iris Y position: 0.3-0.7 (not top/bottom scanning)
   - Head lateral angle: < 30Â° from forward
   
3. Aggregate across entire video:
   forward_percent = (frames_with_forward_gaze / total_frames) Ã— 100
   
4. Classification:
   - â‰¥60%: Normal Eye Contact
   - 40-59%: Reduced Eye Contact
   - <40%: Low Eye Contact
```

**Output Score:** 0-100% (higher = more eye contact)

#### 2. Head Stimming (Measurement: Present/Absent)

**Concept:**
Repetitive head movements (side-to-side shaking, nodding, tilting) are stereotyped behaviors common in autism. Movements are typically:
- Confined to small area
- Repetitive at 1-3 Hz frequency
- Self-directed (not responsive)

**Detection Method:**
- Full-body pose estimation extracts head position
- Track movement across frames
- Use FFT (Fast Fourier Transform) for periodicity detection

**Algorithm:**
```
1. Extract head position from pose landmarks:
   head_center = average(left_eye, right_eye, nose)
   
2. Calculate head velocity:
   For each frame: velocity[i] = distance(head[i] - head[i-1])
   
3. Analyze movement patterns:
   - Calculate movement extent (how much area covered)
   - Calculate velocity range
   - Use FFT to detect oscillation frequencies
   
4. Periodicity Detection (FFT):
   fft_result = FFT(velocity_array)
   Look for peaks in 1-3 Hz range (stimming frequencies)
   periodicity_ratio = highest_peak_power / average_noise_power
   
5. Spatial Constraint:
   movement_area = max(x) - min(x) Ã— max(y) - min(y)
   Stimming: confined area < 0.04 (4% of frame)
   
6. Classification:
   if movement_area < 0.04 AND periodicity_ratio > 2.0 AND mean_velocity > 0.015:
       check_percentage = (frames_with_stimming / total_frames) Ã— 100
       if check_percentage > 20%:
           return 'Present'
   return 'Absent'
```

**Output:** Present/Absent (Binary)

#### 3. Hand Stimming (Measurement: Present/Absent)

**Concept:**
Hand movements including flapping, wringing, twisting, or repetitive fidgeting. Autism-related hand stimming is:
- High frequency (rapid movement)
- Repetitive patterns
- Self-directed
- Not communicative

**Detection Method:**
- Hand Pose Estimation: 21 landmarks per hand
- Track wrist (landmark 0) and finger tips for each hand
- Analyze velocity and oscillation patterns

**Algorithm:**
```
1. Extract 21 hand landmarks per hand:
   - Wrist (landmark 0)
   - Fingers (landmarks 1-20)
   - Coordinates: x, y, z (depth)
   
2. Hand Velocity Calculation:
   For wrist position: velocity[i] = distance(wrist[i] - wrist[i-1])
   
3. Flapping Pattern Detection (vertical oscillation):
   hand_height = wrist_y_position
   height_changes = diff(hand_height)
   
   Sign changes = count(where sign changes)
   Vertical_oscillation_rate = sign_changes / total_frames
   
   Expected for flapping: 0.3+ (direction changes frequently)
   
4. Horizontal Movement:
   hand_x = wrist_x_position
   x_changes = diff(hand_x)
   Horizontal_oscillation = sign_changes(x_changes) / total_frames
   
5. Velocity Analysis:
   mean_velocity = average(velocity_array)
   Stimming threshold: > 0.02 per frame
   
6. Classification:
   stimming_frames = 0
   FOR each frame:
       if (mean_velocity > 0.02 AND 
           (vertical_oscillation > 0.3 OR horizontal_oscillation > 0.3)):
           stimming_frames++
   
   stimming_percent = (stimming_frames / total_frames) Ã— 100
   
   if stimming_percent > 15%:
       return 'Present'
   return 'Absent'
```

**Output:** Present/Absent (Binary)

#### 4. Hand Gestures (Measurement: Present/Absent)

**Concept:**
Communicative hand gestures (pointing, waving, reaching) indicate social intent. Autism is associated with reduced use of these gestures. Detection distinguishes between:
- Communicative: Pointing (extended index finger), waving, reaching
- Non-communicative: Closed fists, self-directed movements

**Detection Method:**
- Hand landmarks (21 per hand)
- Hand shape analysis (finger separation, palm openness)
- Spatial location (away from body = reaching)

**Algorithm:**
```
1. Analyze Hand Shape:
   wrist = landmark 0
   index_tip = landmark 8
   pinky_tip = landmark 20
   palm_center = mean(all_landmarks)
   
2. Pointing Detection:
   index_to_pinky_distance = distance(index_tip - pinky_tip)
   index_finger_length = distance(index_tip - index_base)
   
   is_pointing = (index_to_pinky_distance > index_finger_length)
   
3. Open Hand Detection:
   hand_opening = std_dev(distances_from_palm_center)
   is_open = (hand_opening > 0.05) [fingers spread out]
   
4. Reaching Detection:
   hand_distance_from_body = distance(hand - torso_center)
   is_reaching = (hand_distance_from_body > threshold)
   
5. For each frame:
   if (is_pointing OR is_open OR is_reaching):
       communicative_frames++
       
6. Classification:
   gesture_percentage = (communicative_frames / total_frames) Ã— 100
   
   if gesture_percentage > 25%:
       return 'Present'
   return 'Absent'
```

**Output:** Present/Absent (Binary)

#### 5. Social Reciprocity (Measurement: Normal/Low)

**Concept:**
Social reciprocity involves responding to others' social cues and maintaining body posture indicating engagement. Autism often involves reduced reciprocity.

**Detection Method:**
- Full-body pose (33 landmarks)
- Head orientation relative to camera
- Hand position in "communication zone" (shoulder to face height)
- Body orientation analysis

**Algorithm:**
```
1. Extract Key Landmarks:
   left_shoulder = landmark 11
   right_shoulder = landmark 12
   nose = landmark 0
   left_wrist = landmark 15
   right_wrist = landmark 16
   
2. Check Forward-Facing (head toward camera):
   shoulder_center = (left_shoulder + right_shoulder) / 2
   nose_shoulder_distance = |nose.x - shoulder_center.x|
   shoulder_width = distance(right_shoulder - left_shoulder)
   
   is_forward = (nose_shoulder_distance < shoulder_width Ã— 0.2)
   
3. Check Communication Zone (hands up):
   left_engaged = (left_shoulder.y - 0.2 < left_wrist.y < left_shoulder.y + 0.1)
   right_engaged = (right_shoulder.y - 0.2 < right_wrist.y < right_shoulder.y + 0.1)
   
   hands_in_zone = (left_engaged OR right_engaged)
   
4. Engagement Scoring:
   engagement [frame] = is_forward AND hands_in_zone
   
5. Classification:
   engagement_percent = (engaged_frames / total_frames) Ã— 100
   
   if engagement_percent >= 45%:
       return 'Normal'
   return 'Low'
```

**Output:** Normal/Low (Binary)

#### 6. Emotion Variation (Measurement: Normal/Low)

**Concept:**
Restricted range of emotional expressions is associated with autism. This detects variety in facial expressions (joy, sadness, surprise, neutral).

**Detection Method:**
- Facial landmarks and mouth geometry
- Eye opening measurements
- Facial expression classification
- Shannon entropy for diversity calculation

**Algorithm:**
```
1. Extract Expression Landmarks:
   left_eye_top = landmark 159
   left_eye_bottom = landmark 145
   right_eye_top = landmark 386
   right_eye_bottom = landmark 374
   mouth_left = landmark 61
   mouth_right = landmark 291
   mouth_top = landmark 13
   mouth_bottom = landmark 14
   
2. Calculate Metrics:
   left_eye_opening = distance(left_eye_top - left_eye_bottom)
   right_eye_opening = distance(right_eye_top - right_eye_bottom)
   avg_eye_opening = (left + right) / 2
   
   mouth_width = distance(mouth_left - mouth_right)
   mouth_height = distance(mouth_top - mouth_bottom)
   
3. Classify Expression for Each Frame:
   if mouth_height > 0.03 AND mouth_width > 0.08:
       emotion = 'joy'
   elif avg_eye_opening > 0.025 AND mouth_height > 0.025:
       emotion = 'surprise'
   elif mouth_height < 0.01 AND avg_eye_opening < 0.015:
       emotion = 'sadness'
   else:
       emotion = 'neutral'
       
4. Build emotion sequence across video:
   emotion_sequence = [emotions for each frame]
   emotion_counts = count each emotion type
   
5. Shannon Entropy Calculation:
   - Entropy measures diversity
   - Higher entropy = more varied emotions
   - Formula: H = -Î£(p_i Ã— log2(p_i))
   - p_i = probability of emotion i
   
   entropy = calculate_shannon_entropy(emotion_probs)
   unique_emotions = count(emotions with non-zero probability)
   
6. Classification:
   if unique_emotions >= 3 AND entropy > 1.0:
       return 'Normal'
   return 'Low'
```

**Output:** Normal/Low (Binary)

---

## Technical Implementation

### Backend Architecture

#### Express Server Structure
```javascript
// server.js - Main server entry point
const express = require('express');
const mongoose = require('mongoose');
const cors = require('cors');
const dotenv = require('dotenv');

dotenv.config();

const app = express();

// Middleware
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// Authentication Middleware
app.use('/api/protected', authMiddleware);

// Routes
app.use('/api/auth', authRoutes);
app.use('/api/children', childRoutes);
app.use('/api/screenings', screeningRoutes);

// Database Connection
mongoose.connect(process.env.MONGODB_URI, {
  useNewUrlParser: true,
  useUnifiedTopology: true
});

// Error Handling
app.use(errorHandler);

const PORT = process.env.PORT || 5001;
app.listen(PORT, () => console.log(`Server running on port ${PORT}`));
```

#### Authentication Flow
```javascript
// Authentication Middleware
const authMiddleware = async (req, res, next) => {
  const token = req.headers.authorization?.split(' ')[1];
  
  if (!token) {
    return res.status(401).json({ error: 'No token provided' });
  }
  
  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET);
    req.userId = decoded.userId;
    next();
  } catch (error) {
    res.status(401).json({ error: 'Invalid token' });
  }
};

// Login Response
{
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "user": {
    "userId": "507f1f77bcf86cd799439011",
    "name": "Jane Parent",
    "email": "jane@example.com"
  },
  "expiresIn": "7d"
}
```

#### 4-Step Screening Workflow
```javascript
// Step 1: Initialize Screening
POST /api/screenings/start
{
  "childId": "child_mongo_id"
}
// Creates: screeningId, initializes record with status: 'questionnaire_pending'

// Step 2: Submit Questionnaire
POST /api/screenings/:screeningId/questions
{
  "responses": [
    { "questionId": 1, "answer": true },
    // ... 20 total responses
  ]
}
// Calls ML service â†’ Gets mlQuestionnaireScore (0-100%)
// Updates: status: 'video_pending', mlQuestionnaireScore

// Step 3: Upload Video
POST /api/screenings/:screeningId/video
Form Data: { "video": <file> }
// Validates: .mp4, .webm, .mov (size < 500MB, duration 60-300 sec)
// Calls ML service â†’ Gets 6 markers
// Updates: status: 'analysis_pending', videoFeatures

// Step 4: Complete Screening
POST /api/screenings/:screeningId/complete
// Calculates: finalScore = (mlQuestionnaireScore Ã— 0.6) + (videoScore Ã— 0.4)
// Classifies: riskLevel = 'Low'|'Moderate'|'High'
// Calls Groq API â†’ Gets interpretation text
// Generates PDF report
// Updates: status: 'complete', finalScore, riskLevel, interpretation
```

### ML Service Architecture (FastAPI)

#### Video Analysis Pipeline
```python
from fastapi import FastAPI, File, UploadFile
import cv2
import mediapipe as mp
import numpy as np

app = FastAPI()

# Initialize MediaPipe
mp_holistic = mp.solutions.holistic

@app.post("/analyze-video")
async def analyze_video(video: UploadFile = File(...)):
    """
    Main video analysis endpoint
    Returns: { eyeContact, headStimming, handStimming, 
               handGesture, socialReciprocity, emotionVariation }
    """
    
    # 1. Read video file
    video_bytes = await video.read()
    video_array = np.frombuffer(video_bytes, np.uint8)
    
    # 2. Extract frames using OpenCV
    frames = extract_frames(video_array)  # List of numpy arrays
    
    # 3. Process each frame through MediaPipe
    features = []
    
    with mp_holistic.Holistic(
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5
    ) as holistic:
        
        for frame in frames:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = holistic.process(frame_rgb)
            
            # Extract landmarks
            pose_landmarks = results.pose_landmarks
            hand_landmarks = {
                'left': results.left_hand_landmarks,
                'right': results.right_hand_landmarks
            }
            face_landmarks = results.face_landmarks
            
            features.append({
                'pose': pose_landmarks,
                'hands': hand_landmarks,
                'face': face_landmarks
            })
    
    # 4. Detect 6 behavioral markers
    results = {
        'eyeContact': detect_eye_contact(features),
        'headStimming': detect_head_stimming(features),
        'handStimming': detect_hand_stimming(features),
        'handGesture': detect_hand_gesture(features),
        'socialReciprocity': detect_social_reciprocity(features),
        'emotionVariation': detect_emotion_variation(features),
        'sessionDuration': len(frames) / 30,  # 30 FPS
        'totalFrames': len(frames)
    }
    
    return results
```

#### ML Model Training
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
import joblib

# Training Data
X_train = load_questionnaire_responses()  # Shape: (n_samples, 20)
y_train = load_autism_labels()            # Shape: (n_samples,) binary

# Encode with reverse-coding for Q2, Q5, Q12
X_train_encoded = encode_questionnaire(X_train, reverse_questions=[2, 5, 12])

# Create Ensemble
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Train both models
rf_model.fit(X_train_encoded, y_train)
gb_model.fit(X_train_encoded, y_train)

# Ensemble Prediction
def predict_autism_probability(questionnaire_responses):
    X_encoded = encode_questionnaire(questionnaire_responses, 
                                    reverse_questions=[2, 5, 12])
    
    rf_prob = rf_model.predict_proba(X_encoded)[:, 1]
    gb_prob = gb_model.predict_proba(X_encoded)[:, 1]
    
    # Average ensemble
    avg_prob = (rf_prob + gb_prob) / 2
    
    return avg_prob * 100  # Convert to 0-100%
```

### Frontend Implementation

#### React Component Hierarchy
```
App/
â”œâ”€â”€ AuthContext (JWT token management)
â”œâ”€â”€ Router
â”‚   â”œâ”€â”€ HomePage
â”‚   â”‚   â”œâ”€â”€ Hero
â”‚   â”‚   â”œâ”€â”€ About (explains 60/40 methodology)
â”‚   â”‚   â”œâ”€â”€ HowItWorks (5-step process)
â”‚   â”‚   â””â”€â”€ CTA
â”‚   â”œâ”€â”€ LoginPage
â”‚   â”œâ”€â”€ RegisterPage
â”‚   â”œâ”€â”€ Dashboard
â”‚   â”‚   â”œâ”€â”€ ChildProfileCard[]
â”‚   â”‚   â”œâ”€â”€ LatestScreeningCard
â”‚   â”‚   â””â”€â”€ HistoryLink
â”‚   â”œâ”€â”€ ScreeningFlow
â”‚   â”‚   â”œâ”€â”€ ProgressBar
â”‚   â”‚   â”œâ”€â”€ QuestionnaireForm (20 M-CHAT-R questions)
â”‚   â”‚   â”‚   â””â”€â”€ Question[] (yes/no toggle)
â”‚   â”‚   â””â”€â”€ VideoUploader
â”‚   â”‚       â”œâ”€â”€ VideoRecorder (webcam)
â”‚   â”‚       â””â”€â”€ FileUpload
â”‚   â””â”€â”€ ScreeningResults
â”‚       â”œâ”€â”€ ScoreVisualization
â”‚       â”‚   â”œâ”€â”€ FinalScore (big number)
â”‚       â”‚   â”œâ”€â”€ RiskLevel (badge)
â”‚       â”‚   â”œâ”€â”€ ComponentScores (questionnaire + video)
â”‚       â”‚   â””â”€â”€ VideoMarkersBreakdown (6 markers)
â”‚       â”œâ”€â”€ Interpretation (LLM text)
â”‚       â””â”€â”€ DownloadPDF
```

#### Questionnaire Component
```jsx
function ScreeningFlow() {
  const [step, setStep] = useState(0); // 0: questions, 1: video
  const [responses, setResponses] = useState({});
  const [videoFile, setVideoFile] = useState(null);
  
  const mchatQuestions = [
    {
      id: 1,
      text: "If you point at something across the room, does your child look at it?",
      example: "For example, if you point at a toy or dog, does your child look at the toy or dog?",
      reverse: false
    },
    {
      id: 2,
      text: "Have you ever wondered if your child might be deaf?",
      example: "For example, do they react to very loud noises?",
      reverse: true  // YES indicates concern
    },
    // ... all 20 questions
  ];
  
  const handleQuestion = (questionId, answer) => {
    setResponses(prev => ({
      ...prev,
      [questionId]: answer
    }));
  };
  
  const handleSubmitQuestionnaire = async () => {
    const response = await axios.post(
      `/api/screenings/${screeningId}/questions`,
      { responses }
    );
    setStep(1); // Move to video
  };
  
  return (
    <div>
      {step === 0 && (
        <div>
          <h2>M-CHAT-R Questionnaire (20 questions)</h2>
          {mchatQuestions.map(q => (
            <QuestionCard
              key={q.id}
              question={q}
              onAnswer={(answer) => handleQuestion(q.id, answer)}
            />
          ))}
          <button onClick={handleSubmitQuestionnaire}>Next: Video Upload</button>
        </div>
      )}
      
      {step === 1 && (
        <VideoUploader
          onVideoSelect={setVideoFile}
          onSubmit={handleSubmitVideo}
        />
      )}
    </div>
  );
}
```

---

## Data Management

### MongoDB Schema Structure

```javascript
// User Collection
{
  _id: ObjectId,
  name: String,
  email: String (unique),
  passwordHash: String, // bcryptjs hashed
  phoneNumber: String,
  createdAt: Date,
  lastLogin: Date,
  isActive: Boolean,
  preferences: {
    language: String,
    notifyEmail: Boolean
  }
}

// Child Collection
{
  _id: ObjectId,
  userId: ObjectId (ref: User),
  name: String,
  nickname: String,
  ageInMonths: Number,
  gender: String, // 'male', 'female', 'other'
  profileImageUrl: String,
  dateOfBirth: Date,
  demographics: {
    ethnicity: String,
    hasASDFamily: Boolean,
    developmentalConcerns: [String]
  },
  createdAt: Date,
  lastUpdated: Date
}

// Screening Collection
{
  _id: ObjectId,
  userId: ObjectId (ref: User),
  childId: ObjectId (ref: Child),
  status: String, // 'started', 'questionnaire_complete', 'video_complete', 'complete'
  
  // Questionnaire Data
  questionnaire: {
    completed: Boolean,
    responses: [
      {
        questionId: Number (1-20),
        question: String,
        answer: Boolean,
        reverse: Boolean
      }
    ],
    score: Number,
    jaundice: Boolean,
    familyASD: Boolean
  },
  
  // ML Questionnaire Score
  mlQuestionnaireScore: Number, // 0-100 (from ensemble model)
  
  // Video Data
  videoFile: {
    filename: String,
    uploadedAt: Date,
    duration: Number, // seconds
    fileSize: Number // bytes
  },
  
  // Live Video Features
  liveVideoFeatures: {
    eyeContact: Number, // 0-100%
    headStimming: Boolean,
    handStimming: Boolean,
    handGesture: Boolean,
    socialReciprocity: String, // 'Normal' or 'Low'
    emotionVariation: String   // 'Normal' or 'Low'
  },
  
  // Video Analysis Results
  videoScore: Number, // 0-100 (calculated from 6 markers)
  sessionDuration: Number, // seconds
  totalFrames: Number,
  
  // Final Results
  finalScore: Number, // 0-100 (60% quest + 40% video)
  riskLevel: String, // 'Low' (<40), 'Moderate' (40-70), 'High' (â‰¥70)
  
  // LLM Interpretation
  interpretation: {
    summary: String,
    confidence: Number, // 0-100
    videoBehaviorAnalysis: String,
    questionnaireSummary: String,
    recommendations: [String],
    nextSteps: String,
    llmAnalysis: String
  },
  
  timestamp: Date,
  completedAt: Date
}
```

### Data Access Patterns

1. **User Isolation**: Queries always include `userId` filter
2. **Child Security**: Verify `userId` matches before returning child data
3. **Screening Privacy**: Only owner can access screening results
4. **Indexing**: Indexes on userId, childId, timestamp for performance

---

## Deployment & Operations

### Environment Configuration

#### Backend (.env)
```env
# Server
PORT=5001
NODE_ENV=production

# Database
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/db?retryWrites=true

# Authentication
JWT_SECRET=<random 64+ character string>
JWT_EXPIRE=7d

# External Services
ML_SERVICE_URL=http://ml-service:8000
GROQ_API_KEY=<from console.groq.com>

# File Upload
UPLOAD_MAX_SIZE=524288000  # 500MB
ALLOWED_VIDEO_FORMATS=mp4,webm,mov

# Logging
LOG_LEVEL=info
```

#### ML Service (.env)
```env
PORT=8000
GROQ_API_KEY=<from console.groq.com>
MODEL_CONFIDENCE_THRESHOLD=0.7
VIDEO_FPS=30
MEDIAPIPE_MIN_DETECTION_CONFIDENCE=0.7
```

#### Frontend (.env)
```env
VITE_API_URL=https://api.autisense.app
VITE_PORT=5173
```

### Docker Deployment

```dockerfile
# Backend Dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package.json .
RUN npm ci --only=production
COPY . .
EXPOSE 5001
CMD ["npm", "start"]

# ML Service Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["python", "main.py"]

# Frontend Dockerfile
FROM node:18-alpine as builder
WORKDIR /app
COPY package.json .
RUN npm ci
COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

### Kubernetes Deployment (Production)

```yaml
# Backend Service
apiVersion: v1
kind: Service
metadata:
  name: autisense-backend
spec:
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 5001
      targetPort: 5001
  type: LoadBalancer

---
# ML Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-service
  template:
    metadata:
      labels:
        app: ml-service
    spec:
      containers:
      - name: ml-service
        image: autisense/ml-service:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

---

## Security & Compliance

### Authentication & Authorization

**JWT-Based Authentication:**
- Tokens issued on login (7-day expiration)
- Tokens verified on protected routes
- Refresh tokens for session extension
- Logout invalidates token

**Password Security:**
- Bcryptjs hashing (12 salt rounds)
- No passwords stored in plaintext
- Minimum 8 characters required
- Special characters recommended

**Authorization:**
- Role-based access control (parent, clinician)
- User isolation at database level
- Permission checks on every API call

### Data Protection

**In Transit:**
- HTTPS/TLS 1.3+ for all connections
- Secure WebSocket (WSS) for real-time
- Certificate pinning recommended
- HSTS headers enforced

**At Rest:**
- MongoDB encryption enabled
- File storage encrypted
- Database backups encrypted
- No sensitive data in logs

###Compliance

- **HIPAA**: Can be deployed HIPAA-compliant
- **GDPR**: Data deletion, export, privacy controls
- **COPPA**: Child data protection (users are parents)
- **Accessibility**: WCAG 2.1 AA compliance
- **Standards**: HL7 compatibility for health integration

### Privacy Features

- **Data Minimization**: Only collect necessary information
- **User Control**: Users manage their own data
- **Consent**: Explicit consent for assessment use
- **Deletion**: Right to delete screening history
- **Anonymization**: Option for research use

---

## Important Limitations & Disclaimers

### What Autisense is NOT:
- âŒ Not a diagnostic tool (screening only)
- âŒ Not a replacement for professional evaluation
- âŒ Not suitable as standalone assessment
- âŒ Should not delay professional consultation

### Key Limitations:
1. **Video Quality**: Depends on lighting, camera, internet
2. **Age Range**: Designed for 12-72 months
3. **Language**: Currently English only
4. **Accuracy**: 88-93% accuracy rates on markers
5. **Environmental Factors**: Home screening may miss clinical context

### Recommended Use:
âœ… **Primary**: Screening tool for parents to identify need for evaluation  
âœ… **Secondary**: Clinical decision-support tool  
âœ… **Tertiary**: Tracking tool between professional evaluations  
âš ï¸ **Never**: Sole basis for major decisions about child's care  

---

## Future Roadmap

### Phase 2 (Q2-Q3 2026)
- [ ] Mobile app (React Native)
- [ ] Video upload with cloud storage
- [ ] Multi-language support (Spanish, Mandarin)
- [ ] Therapist dashboard
- [ ] Integration with health records systems

### Phase 3 (Q4 2026 - Q1 2027)
- [ ] Advanced ML models (custom CNN)
- [ ] Real-time video screening (no file upload)
- [ ] Longitudinal tracking (months/years)
- [ ] Telehealth integration
- [ ] Appointment scheduling with pediatricians

### Phase 4 (2027+)
- [ ] Wearable device integration
- [ ] Audio/speech analysis
- [ ] Family history analysis ML
- [ ] Prediction modeling (future risk)
- [ ] Research data portal
- [ ] International deployment

---

## Technical Support

### Documentation
- **GitHub Repository**: https://github.com/Thejasteju48/Autisense
- **README.md**: Project overview
- **COMPLETE_PROJECT_DOCUMENTATION.md**: Technical deep-dive
- **REQUIREMENTS.md**: Dependency documentation
- **QUESTIONNAIRE_REFERENCE.md**: M-CHAT-R details

### Getting Help
1. Check documentation first
2. Review GitHub issues
3. Contact development team
4. Submit bug reports with:
   - Environment details
   - Steps to reproduce
   - Screenshots/logs

### System Requirements
**Minimum:**
- 4GB RAM
- 2-core processor
- 50 Mbps internet
- Modern browser (Chrome, Firefox, Safari, Edge)

**Recommended:**
- 8GB+ RAM
- 4+ core processor
- 100 Mbps internet
- Latest browser version
- Dedicated GPU for ML service

---

## Conclusion

Autisense represents a significant advancement in autism screening accessibility. By combining validated clinical questionnaires with AI-powered video analysis, it provides a comprehensive, evidence-based assessment tool that can support early identification and intervention.

The system is:
- **Evidence-Based**: Founded on established M-CHAT-R methodology
- **Accessible**: Web-based, no special equipment needed
- **Objective**: AI-powered video analysis removes subjectivity
- **Scalable**: Cloud-native architecture supports growth
- **Secure**: HIPAA-aligned data protection
- **Interpretable**: Results explained clearly to parents

---

**For more information, visit: [GitHub Repository](https://github.com/Thejasteju48/Autisense)**

*Version 1.0.0 | Built with â¤ï¸ for early autism intervention*

